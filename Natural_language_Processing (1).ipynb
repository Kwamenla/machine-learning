{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11G2_TUgZd9-",
        "outputId": "a80914bb-0313-4a74-e59b-91d351ac2156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-02 09:26:41--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  16.4MB/s    in 12s     \n",
            "\n",
            "2023-06-02 09:26:53 (6.59 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "Large Movie Review Dataset v1.0\n",
            "\n",
            "Overview\n",
            "\n",
            "This dataset contains movie reviews along with their associated binary\n",
            "sentiment polarity labels. It is intended to serve as a benchmark for\n",
            "sentiment classification. This document outlines how the dataset was\n",
            "gathered, and how to use the files provided. \n",
            "\n",
            "Dataset \n",
            "\n",
            "The core dataset contains 50,000 reviews split evenly into 25k train\n",
            "and 25k test sets. The overall distribution of labels is balanced (25k\n",
            "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
            "documents for unsupervised learning. \n",
            "\n",
            "In the entire collection, no more than 30 reviews are allowed for any\n",
            "given movie because reviews for the same movie tend to have correlated\n",
            "ratings. Further, the train and test sets contain a disjoint set of\n",
            "movies, so no significant performance is obtained by memorizing\n",
            "movie-unique terms and their associated with observed labels.  In the\n",
            "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
            "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
            "more neutral ratings are not included in the train/test sets. In the\n",
            "unsupervised set, reviews of any rating are included and there are an\n",
            "even number of reviews > 5 and <= 5.\n",
            "\n",
            "Files\n",
            "\n",
            "There are two top-level directories [train/, test/] corresponding to\n",
            "the training and test sets. Each contains [pos/, neg/] directories for\n",
            "the reviews with binary labels positive and negative. Within these\n",
            "directories, reviews are stored in text files named following the\n",
            "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
            "the star rating for that review on a 1-10 scale. For example, the file\n",
            "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
            "example with unique id 200 and star rating 8/10 from IMDb. The\n",
            "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
            "omitted for this portion of the dataset.\n",
            "\n",
            "We also include the IMDb URLs for each review in a separate\n",
            "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
            "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
            "are unable to link directly to the review, but only to the movie's\n",
            "review page.\n",
            "\n",
            "In addition to the review text files, we include already-tokenized bag\n",
            "of words (BoW) features that were used in our experiments. These \n",
            "are stored in .feat files in the train/test directories. Each .feat\n",
            "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
            "data.  The feature indices in these files start from 0, and the text\n",
            "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
            "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
            "(the) appears 7 times in that review.\n",
            "\n",
            "LIBSVM page for details on .feat file format:\n",
            "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
            "\n",
            "We also include [imdbEr.txt] which contains the expected rating for\n",
            "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
            "rating is a good way to get a sense for the average polarity of a word\n",
            "in the dataset.\n",
            "\n",
            "Citing the dataset\n",
            "\n",
            "When using this dataset please cite our ACL 2011 paper which\n",
            "introduces it. This paper also contains classification results which\n",
            "you may want to compare against.\n",
            "\n",
            "\n",
            "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "  month     = {June},\n",
            "  year      = {2011},\n",
            "  address   = {Portland, Oregon, USA},\n",
            "  publisher = {Association for Computational Linguistics},\n",
            "  pages     = {142--150},\n",
            "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "}\n",
            "\n",
            "References\n",
            "\n",
            "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
            "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
            "636-659.\n",
            "\n",
            "Contact\n",
            "\n",
            "For questions/comments/corrections please contact Andrew Maas\n",
            "amaas@cs.stanford.edu\n"
          ]
        }
      ],
      "source": [
        "#Download IMDB to the current folder\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# Unzip\n",
        "!tar zxf aclImdb_v1.tar.gz\n",
        "# aclImdb / train / unsup is unlabeled and removed\n",
        "!rm -rf aclImdb/train/unsup\n",
        "# Show IMDB dataset description\n",
        "!cat aclImdb/README"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_files\n",
        "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
        "x_train, y_train = train_review.data, train_review.target\n",
        "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
        "x_test, y_test = test_review.data, test_review.target\n",
        "# Display of the correspondence between 0, 1 of the label and the meaning\n",
        "print(train_review.target_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WIKtKshZhIs",
        "outputId": "db11427a-cc85-422d-b62a-4507fb77d6b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['neg', 'pos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x : {}\".format(x_train[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwBhg4D4ZhN0",
        "outputId": "e81f0866-f64b-4df7-9e18-d4f67e7849bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x : Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mini_dataset = \\\n",
        "  [\"This movie is very good.\",\n",
        "  \"This film is a good\",\n",
        "  \"Very bad. Very, very bad.\"]"
      ],
      "metadata": {
        "id": "Rt-vCtvwZhUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
        "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "# Put together in DataFrame\n",
        "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "HTjwOOpGZ4k1",
        "outputId": "848b895c-d1a1-446c-af4b-acdca8bd795c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3b6ef18b67f5>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Put together in DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the range of n-gram used in ngram_range\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "metadata": {
        "id": "A3p3emPKZ4qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_line = 'This movie is SOOOO funny!!!'.lower().replace('!', '').split()\n",
        "second_line = 'What a movie! I never'.lower().replace('!', '').split()\n",
        "third_line = 'best movie ever!!!!! this movie'.lower().replace('!', '').split()"
      ],
      "metadata": {
        "id": "6xFTELAeZ4wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gram_1_feature_names = first_line + second_line + third_line"
      ],
      "metadata": {
        "id": "2cWmSuu9Z43N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "gram_1 = pd.DataFrame(np.zeros((3, len(set(gram_1_feature_names)))).astype('int'), columns=set(gram_1_feature_names))\n",
        "\n",
        "for i, ss in enumerate([first_line, second_line, third_line]):\n",
        "    for s in ss:\n",
        "        n = ss.count(s)\n",
        "        gram_1[s][i] = n\n",
        "\n",
        "gram_1"
      ],
      "metadata": {
        "id": "vJBZbWkwZ4-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gram_2_feature_names = []\n",
        "gram_2_lines = []\n",
        "for i, s in enumerate([first_line, second_line, third_line]):\n",
        "  line = []\n",
        "  for ss in range(len(s)-1):\n",
        "    line.append(f'{s[ss]} {s[ss+1]}')\n",
        "    gram_2_feature_names.append(f'{s[ss]} {s[ss+1]}')\n",
        "  gram_2_lines.append(line)"
      ],
      "metadata": {
        "id": "FRfCRCmaZhdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gram_2 = pd.DataFrame(np.zeros((3, len(set(gram_2_feature_names)))).astype('int'), columns=set(gram_2_feature_names))\n",
        "\n",
        "for i, ss in enumerate(gram_2_lines):\n",
        "    for s in ss:\n",
        "        n = ss.count(s)\n",
        "        gram_2[s][i] = n\n",
        "        \n",
        "gram_2"
      ],
      "metadata": {
        "id": "5OsQFWOGZhiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "n_samples = 25000\n",
        "idf = np.log(n_samples/np.arange(1,n_samples))\n",
        "plt.title(\"IDF\")\n",
        "plt.xlabel(\"df(t)\")\n",
        "plt.ylabel(\"IDF\")\n",
        "plt.plot(idf)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KhOkAlr8aaGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "metadata": {
        "id": "6tcyJzqoaaLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Stopword for the first time\n",
        "import nltk\n",
        "stop_words = nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
      ],
      "metadata": {
        "id": "9ZbgxjAnaaSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "metadata": {
        "id": "9PalMhtnamoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words= stop_words, max_features=5000)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "X_test = vectorizer.fit_transform(x_test)"
      ],
      "metadata": {
        "id": "qLhtxL2pamt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "sV6F3B3sam0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "lgb = lgb.LGBMClassifier().fit(X_train,y_train)\n",
        "y_pred = lgb.predict(X_test)"
      ],
      "metadata": {
        "id": "hf6Cb2xTam6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"{}\".format(lgb.score(X_test, y_test)))\n",
        "print(\"{}\".format(precision_score(y_test,y_pred)))\n",
        "print(\"{}\".format(recall_score(y_test,y_pred)))\n",
        "print(\"{}\".format(f1_score(y_test,y_pred)))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "QeQzUlksanA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
        "model = Word2Vec(min_count=1, size=10) # Set the number of dimensions to 10\n",
        "model.build_vocab(sentences) # Preparation\n",
        "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) #Learning\n",
        "\n",
        "print(\"List of vocabulary : {}\".format(model.wv.vocab.keys()))\n",
        "for vocab in model.wv.vocab.keys():\n",
        "  print(\"Vector of {}: \\n{}\".format(vocab, model.wv[vocab]))"
      ],
      "metadata": {
        "id": "Kp5ntQHea7Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=\"good\", topn=3)"
      ],
      "metadata": {
        "id": "U6ewhjBca7Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "vocabs = model.wv.vocab.keys()\n",
        "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
        "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
        "for i, word in enumerate(list(vocabs)):\n",
        "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
        "ax.set_yticklabels([])\n",
        "ax.set_xticklabels([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N1N_PM2pa7d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = Word2Vec(min_count=1, size=10) # Set the number of dimensions to 10\n",
        "model_2.build_vocab(x_train) # Preparation\n",
        "model_2.train(x_train, total_examples=model_2.corpus_count, epochs=model_2.iter) #Learning"
      ],
      "metadata": {
        "id": "_TBdS6ZBbG26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_url = 0\n",
        "for i, s in enumerate(x_train):\n",
        "    if 'www' in s:\n",
        "      with_url = i\n",
        "      print('before processing')\n",
        "      print('-----')\n",
        "      print(s)\n",
        "      break"
      ],
      "metadata": {
        "id": "CByYrmcJbHA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "no_preprocessing = x_train[with_url]\n",
        "after_preprocessing1 = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", no_preprocessing) \n",
        "after_preprocessing2 = re.sub(r'<[^>]+>', \" \", after_preprocessing1) \n",
        "after_preprocessing3 = re.sub(r\"[^0-9a-zA-Z ]\", \"\", after_preprocessing2) \n",
        "after_preprocessing = after_preprocessing3.lower() \n",
        "print('after processing')\n",
        "print('-----')\n",
        "print(after_preprocessing)"
      ],
      "metadata": {
        "id": "So9qG3VZbNyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(min_count=1, size=10) \n",
        "model.build_vocab(after_preprocessing) \n",
        "model.train(after_preprocessing, total_examples=model.corpus_count, epochs=model.iter)"
      ],
      "metadata": {
        "id": "b1VN_3vybN4-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}